{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gegem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyXtyeoWCAm18t1XIYI79s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coll-j/IndonesianDepParse/blob/master/gegem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSZwJIfqwu4C",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzPVzPc1u7qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ez8pCmwt6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "a0f04078-36f0-4442-82c6-fe426690866e"
      },
      "source": [
        "%%bash\n",
        "wget https://github.com/coll-j/IndonesianDepParse/blob/master/test.txt\n",
        "pip install nlp-id"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlp-id\n",
            "  Downloading https://files.pythonhosted.org/packages/08/10/afe2f49703d27600292130b5308139018231ae0b204ca12c33907f8294bb/nlp_id-0.1.9.8.tar.gz (7.5MB)\n",
            "Collecting scikit-learn==0.22\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "Collecting wget==3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22->nlp-id) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22->nlp-id) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22->nlp-id) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->nlp-id) (1.15.0)\n",
            "Building wheels for collected packages: nlp-id, nltk, wget\n",
            "  Building wheel for nlp-id (setup.py): started\n",
            "  Building wheel for nlp-id (setup.py): finished with status 'done'\n",
            "  Created wheel for nlp-id: filename=nlp_id-0.1.9.8-cp36-none-any.whl size=7723018 sha256=7f66d4f55cc59e6dce7bf18c144c142ebdc517f35f58ad105fb9efc921c34edf\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/04/4e/25d7de62878a993a6642138ef4c9b5ea2cdc78f12cf8624c4d\n",
            "  Building wheel for nltk (setup.py): started\n",
            "  Building wheel for nltk (setup.py): finished with status 'done'\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449905 sha256=6e15a0f0707452164c94e18cd40754b8dcca00ab44b6027aadcf6b4f2ffa02c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for wget (setup.py): started\n",
            "  Building wheel for wget (setup.py): finished with status 'done'\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=16e2dd59555f921341b21db1778ba77a550d2f1ea670552a39cb4ad7c841b5d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built nlp-id nltk wget\n",
            "Installing collected packages: scikit-learn, nltk, wget, nlp-id\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nlp-id-0.1.9.8 nltk-3.4.5 scikit-learn-0.22 wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-08-23 13:08:01--  https://github.com/coll-j/IndonesianDepParse/blob/master/test.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  920K\n",
            "    50K .......... .......... .........                        1.37M=0.08s\n",
            "\n",
            "2020-08-23 13:08:02 (1.03 MB/s) - ‘test.txt’ saved [81761]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JCB2F4159sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7966630-317c-47b2-d821-ef17397a8f8b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nlp_id.tokenizer import Tokenizer\n",
        "from nlp_id.postag import PosTag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7FER5WUDf_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIELD_TO_IDX = {'idx': 0, 'word': 1, 'postag': 2, 'head': 3, 'deprel': 4}\n",
        "def get_long_tensor(tokens_list, batch_size):\n",
        "    \"\"\" Convert (list of )+ tokens to a padded LongTensor. \"\"\"\n",
        "    sizes = []\n",
        "    x = tokens_list\n",
        "    size = (max(len(y) for y in x))\n",
        "    tokens = torch.LongTensor(batch_size, size).fill_(0)\n",
        "    for i, s in enumerate(tokens_list):\n",
        "      ten = torch.LongTensor(list(map(ord, s[:len(s)])))\n",
        "      tokens[i, :len(s)] = ten\n",
        "    return tokens"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh930kSmAXFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loader:\n",
        "  \n",
        "  def __init__(self, file_path, batch_size):\n",
        "    self._file_path = file_path\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.data = self.load_file()\n",
        "    self.vocabs = self.get_vocabs(self.data)\n",
        "    self.data = self.chunk_batches(self.data)\n",
        "\n",
        "  def load_file(self):\n",
        "    sents, sent = [], []\n",
        "    f = open(self._file_path)\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if len(line) == 0:\n",
        "        if len(sent) > 0:\n",
        "          sents.append(sent)\n",
        "          sent = []\n",
        "\n",
        "      else:\n",
        "        cols = line.split('\\t')\n",
        "        sent += [cols]\n",
        "    if len(sent) > 0:\n",
        "      sents.append(sent)\n",
        "\n",
        "    return sents\n",
        "\n",
        "  def get_vocabs(self, data):\n",
        "    words = [cols[FIELD_TO_IDX['word']] for sent in data for cols in sent]\n",
        "    postags = [cols[FIELD_TO_IDX['postag']] for sent in data for cols in sent] \n",
        "    deprels = [cols[FIELD_TO_IDX['deprel']] for sent in data for cols in sent] \n",
        "\n",
        "    words = self.build_vocab(words)\n",
        "    postags = self.build_vocab(postags)\n",
        "    deprels = self.build_vocab(deprels)\n",
        "\n",
        "    return {'words': words, 'postags': postags, 'deprels': deprels}\n",
        "\n",
        "  def build_vocab(self, cols):\n",
        "    w2i = {}\n",
        "    for w in cols:\n",
        "      if w not in w2i:\n",
        "        w2i[w] = len(w2i)\n",
        "    \n",
        "    return w2i\n",
        "\n",
        "  def chunk_batches(self, data):\n",
        "    # TO DO: Chunk data into batches\n",
        "    res, curr = [], []\n",
        "    currlen = 0\n",
        "    for sent in data:\n",
        "      if len(sent[0]) + currlen > self.batch_size:\n",
        "        if len(curr) > 0:\n",
        "          res.append(curr)\n",
        "          curr = []\n",
        "          currlen = 0\n",
        "      \n",
        "      curr.append(sent)\n",
        "      currlen += len(sent[0])\n",
        "    if len(curr) > 0:\n",
        "      res.append(curr)\n",
        "\n",
        "    return res\n",
        "    \n",
        "  def get_batches(self, key):\n",
        "    print('data[i]', self.data[key])\n",
        "    batch = self.data[key]\n",
        "    # convert to tensors\n",
        "    words = [cols[FIELD_TO_IDX['word']] for sent in batch for cols in sent]\n",
        "    words = get_long_tensor(words, len(words))\n",
        "    words_mask = torch.eq(words, 0)\n",
        "\n",
        "    postags = [cols[FIELD_TO_IDX['postag']] for sent in batch for cols in sent]\n",
        "    postags = get_long_tensor(postags, len(postags))\n",
        "    heads = [cols[FIELD_TO_IDX['head']] for sent in batch for cols in sent]\n",
        "    heads = get_long_tensor(heads, len(heads))\n",
        "    deprels = [cols[FIELD_TO_IDX['deprel']] for sent in batch for cols in sent]\n",
        "    deprels = get_long_tensor(deprels, len(deprels))\n",
        "    sentlens = [len(y) for y in batch]\n",
        "    print('sentlens', sentlens)\n",
        "\n",
        "    return words, words_mask, postags, heads, deprels, sentlens\n",
        "\n",
        "  def __iter__(self):\n",
        "    for i in range(len(self.data)):\n",
        "      yield self.get_batches(i)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHafl7MzU6AM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "926d0ca3-ad47-4c31-e71d-e9103105d569"
      },
      "source": [
        "data = Loader('test.txt', 11)\n",
        "for d in data:\n",
        "  print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data[i] [[['1', 'Banyak', 'NUM', '2', 'nummod'], ['2', 'buku', 'NN', '5', 'obj'], ['3', 'telah', 'ADV', '5', 'advmod'], ['4', 'saya', 'PR', '5', 'nsubj'], ['5', 'baca', 'VB', '0', 'root']], [['1', 'Buku', 'NN', '4', 'nsubj'], ['2', 'ini', 'PR', '1', 'nmod'], ['3', 'telah', 'ADV', '4', 'advmod'], ['4', 'dibaca', 'VB', '0', 'root']]]\n",
            "sentlens [5, 4]\n",
            "(tensor([[ 66,  97, 110, 121,  97, 107],\n",
            "        [ 98, 117, 107, 117,   0,   0],\n",
            "        [116, 101, 108,  97, 104,   0],\n",
            "        [115,  97, 121,  97,   0,   0],\n",
            "        [ 98,  97,  99,  97,   0,   0],\n",
            "        [ 66, 117, 107, 117,   0,   0],\n",
            "        [105, 110, 105,   0,   0,   0],\n",
            "        [116, 101, 108,  97, 104,   0],\n",
            "        [100, 105,  98,  97,  99,  97]]), tensor([[False, False, False, False, False, False],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False,  True,  True,  True],\n",
            "        [False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False]]), tensor([[78, 85, 77],\n",
            "        [78, 78,  0],\n",
            "        [65, 68, 86],\n",
            "        [80, 82,  0],\n",
            "        [86, 66,  0],\n",
            "        [78, 78,  0],\n",
            "        [80, 82,  0],\n",
            "        [65, 68, 86],\n",
            "        [86, 66,  0]]), tensor([[50],\n",
            "        [53],\n",
            "        [53],\n",
            "        [53],\n",
            "        [48],\n",
            "        [52],\n",
            "        [49],\n",
            "        [52],\n",
            "        [48]]), tensor([[110, 117, 109, 109, 111, 100],\n",
            "        [111,  98, 106,   0,   0,   0],\n",
            "        [ 97, 100, 118, 109, 111, 100],\n",
            "        [110, 115, 117,  98, 106,   0],\n",
            "        [114, 111, 111, 116,   0,   0],\n",
            "        [110, 115, 117,  98, 106,   0],\n",
            "        [110, 109, 111, 100,   0,   0],\n",
            "        [ 97, 100, 118, 109, 111, 100],\n",
            "        [114, 111, 111, 116,   0,   0]]), [5, 4])\n",
            "data[i] [[['1', 'Kita', 'PR', '3', 'nsubj'], ['2', 'sedang', 'ADV', '3', 'advmod'], ['3', 'berada', 'VB', '0', 'root'], ['4', 'di', 'IN', '6', 'appos'], ['5', 'dalam', 'NN', '6', 'advmod'], ['6', 'ruangan', 'NN', '3', 'obj'], ['7', '.', 'SYM', '3', 'punct']]]\n",
            "sentlens [7]\n",
            "(tensor([[ 75, 105, 116,  97,   0,   0,   0],\n",
            "        [115, 101, 100,  97, 110, 103,   0],\n",
            "        [ 98, 101, 114,  97, 100,  97,   0],\n",
            "        [100, 105,   0,   0,   0,   0,   0],\n",
            "        [100,  97, 108,  97, 109,   0,   0],\n",
            "        [114, 117,  97, 110, 103,  97, 110],\n",
            "        [ 46,   0,   0,   0,   0,   0,   0]]), tensor([[False, False, False, False,  True,  True,  True],\n",
            "        [False, False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False,  True],\n",
            "        [False, False,  True,  True,  True,  True,  True],\n",
            "        [False, False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False, False, False],\n",
            "        [False,  True,  True,  True,  True,  True,  True]]), tensor([[80, 82,  0],\n",
            "        [65, 68, 86],\n",
            "        [86, 66,  0],\n",
            "        [73, 78,  0],\n",
            "        [78, 78,  0],\n",
            "        [78, 78,  0],\n",
            "        [83, 89, 77]]), tensor([[51],\n",
            "        [51],\n",
            "        [48],\n",
            "        [54],\n",
            "        [54],\n",
            "        [51],\n",
            "        [51]]), tensor([[110, 115, 117,  98, 106,   0],\n",
            "        [ 97, 100, 118, 109, 111, 100],\n",
            "        [114, 111, 111, 116,   0,   0],\n",
            "        [ 97, 112, 112, 111, 115,   0],\n",
            "        [ 97, 100, 118, 109, 111, 100],\n",
            "        [111,  98, 106,   0,   0,   0],\n",
            "        [112, 117, 110,  99, 116,   0]]), [7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIkpLDVU-sX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "35b17ea0-1e08-4c36-dbd0-61a36c718857"
      },
      "source": [
        "data.vocabs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'deprels': {'advmod': 2,\n",
              "  'appos': 6,\n",
              "  'nmod': 5,\n",
              "  'nsubj': 3,\n",
              "  'nummod': 0,\n",
              "  'obj': 1,\n",
              "  'punct': 7,\n",
              "  'root': 4},\n",
              " 'postags': {'ADV': 2, 'IN': 5, 'NN': 1, 'NUM': 0, 'PR': 3, 'SYM': 6, 'VB': 4},\n",
              " 'words': {'.': 14,\n",
              "  'Banyak': 0,\n",
              "  'Buku': 5,\n",
              "  'Kita': 8,\n",
              "  'baca': 4,\n",
              "  'berada': 10,\n",
              "  'buku': 1,\n",
              "  'dalam': 12,\n",
              "  'di': 11,\n",
              "  'dibaca': 7,\n",
              "  'ini': 6,\n",
              "  'ruangan': 13,\n",
              "  'saya': 3,\n",
              "  'sedang': 9,\n",
              "  'telah': 2}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWrYuzNo_D2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "9d7796c5-a637-4ab3-e048-98edf956cc59"
      },
      "source": [
        "data.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[['1', 'Banyak', 'NUM', '2', 'nummod'],\n",
              "   ['2', 'buku', 'NN', '5', 'obj'],\n",
              "   ['3', 'telah', 'ADV', '5', 'advmod'],\n",
              "   ['4', 'saya', 'PR', '5', 'nsubj'],\n",
              "   ['5', 'baca', 'VB', '0', 'root']],\n",
              "  [['1', 'Buku', 'NN', '4', 'nsubj'],\n",
              "   ['2', 'ini', 'PR', '1', 'nmod'],\n",
              "   ['3', 'telah', 'ADV', '4', 'advmod'],\n",
              "   ['4', 'dibaca', 'VB', '0', 'root']]],\n",
              " [[['1', 'Kita', 'PR', '3', 'nsubj'],\n",
              "   ['2', 'sedang', 'ADV', '3', 'advmod'],\n",
              "   ['3', 'berada', 'VB', '0', 'root'],\n",
              "   ['4', 'di', 'IN', '6', 'appos'],\n",
              "   ['5', 'dalam', 'NN', '6', 'advmod'],\n",
              "   ['6', 'ruangan', 'NN', '3', 'obj'],\n",
              "   ['7', '.', 'SYM', '3', 'punct']]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EES4WWjFwGO0",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0doyE9pGmCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, args, vocab):\n",
        "    super().__init__()\n",
        "\n",
        "    self.args = args\n",
        "\n",
        "    # input layer\n",
        "    input_size = 0\n",
        "    self.wordemb = nn.Embedding(len(vocab['words']), self.args['word_emb_dim'])\n",
        "    input_size += self.args['word_emb_dim']\n",
        "    self.posemb = nn.Embedding(len(vocab['postags']), self.args['pos_emb_dim'])\n",
        "    input_size += self.args['pos_emb_dim']\n",
        "    self.depemb = nn.Embedding(len(vocab['deprels']), self.args['dep_emb_dim'])\n",
        "\n",
        "    # recurrent layer\n",
        "    self.parserGRU = nn.GRU(input_size, self.args['hidden_dim'], self.args['num_layers'],\\\n",
        "                            batch_first=True, dropout=self.args['dropout'], bidirectional=True)\n",
        "    self.GRU_h0 = nn.Parameter(torch.zeros(self.args['num_layers'] * 2),\\\n",
        "                               self.args['batch_size'], self.args['hidden_dim'])\n",
        "    \n",
        "    # classifier\n",
        "    # TO DO: add MLP and Deep Biaffine for each head and deprel\n",
        "\n",
        "    # criterion\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    self.dropout = nn.Dropout(self.args['dropout'])\n",
        "  def forward(self):\n",
        "    #TODO: make forward func\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzZt0XO12kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}