{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gegem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNaK5ZDF45pKjnfRqVcPC5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coll-j/IndonesianDepParse/blob/master/gegem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSZwJIfqwu4C",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzPVzPc1u7qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSsyhgrUvh_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "73acdddc-d95a-404c-af04-b5df51b49686"
      },
      "source": [
        "a = torch.rand(2, 1, 5)\n",
        "print(\"before\")\n",
        "print(a)\n",
        "a = a.unsqueeze_(-1).expand(2, 1, 5, 5)\n",
        "print(\"after\")\n",
        "print(a)"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before\n",
            "tensor([[[0.5824, 0.7257, 0.9338, 0.1407, 0.9204]],\n",
            "\n",
            "        [[0.2018, 0.5768, 0.7079, 0.4656, 0.3159]]])\n",
            "after\n",
            "tensor([[[[0.5824, 0.5824, 0.5824, 0.5824, 0.5824],\n",
            "          [0.7257, 0.7257, 0.7257, 0.7257, 0.7257],\n",
            "          [0.9338, 0.9338, 0.9338, 0.9338, 0.9338],\n",
            "          [0.1407, 0.1407, 0.1407, 0.1407, 0.1407],\n",
            "          [0.9204, 0.9204, 0.9204, 0.9204, 0.9204]]],\n",
            "\n",
            "\n",
            "        [[[0.2018, 0.2018, 0.2018, 0.2018, 0.2018],\n",
            "          [0.5768, 0.5768, 0.5768, 0.5768, 0.5768],\n",
            "          [0.7079, 0.7079, 0.7079, 0.7079, 0.7079],\n",
            "          [0.4656, 0.4656, 0.4656, 0.4656, 0.4656],\n",
            "          [0.3159, 0.3159, 0.3159, 0.3159, 0.3159]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ez8pCmwt6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c6936630-a2b9-4ec7-ec96-158ec1e56fdb"
      },
      "source": [
        "%%bash\n",
        "rm test.txt\n",
        "wget https://raw.githubusercontent.com/coll-j/IndonesianDepParse/master/test.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'test.txt': No such file or directory\n",
            "--2020-09-02 07:17:10--  https://raw.githubusercontent.com/coll-j/IndonesianDepParse/master/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 316 [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "     0K                                                       100% 13.2M=0s\n",
            "\n",
            "2020-09-02 07:17:10 (13.2 MB/s) - ‘test.txt’ saved [316/316]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JCB2F4159sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7966630-317c-47b2-d821-ef17397a8f8b"
      },
      "source": [
        "pip install nlp-id\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nlp_id.tokenizer import Tokenizer\n",
        "from nlp_id.postag import PosTag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0qSy0b9z8rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'batch_size': 1,\n",
        "    'word_emb_dim': 100,\n",
        "    'pos_emb_dim': 100,\n",
        "    'dep_emb_dim': 100,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 20,\n",
        "    'dropout': 0.33,\n",
        "    'dba_hidden_dim': 128,\n",
        "    'learning_rate': 0.0001,\n",
        "    'clip': 0.25,\n",
        "    'log_interval': 1\n",
        "}\n",
        "FIELD_TO_IDX = {'idx': 0, 'word': 1, 'postag': 2, 'head': 3, 'deprel': 4}"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7FER5WUDf_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "def get_long_tensor(tokens_list, batch_size, vocabs=None):\n",
        "    \"\"\" Convert (list of )+ tokens to a padded LongTensor. \"\"\"\n",
        "    # sizes = []\n",
        "    # for tokens_list in batch:\n",
        "    #   x = tokens_list\n",
        "    #   sizes.append(max(len(y) for y in x))\n",
        "    #   tokens = torch.LongTensor(batch_size, *sizes).fill_(0)\n",
        "    #   for i, s in enumerate(tokens_list):\n",
        "    #     # ten = torch.LongTensor(list(map(ord, s[:len(s)])))\n",
        "    #     if vocabs is not None:\n",
        "    #       ten = torch.LongTensor([vocabs[s]])\n",
        "    #     else:\n",
        "    #       ten = torch.LongTensor([int(s)])\n",
        "    #     tokens[i, :len(s)] = ten\n",
        "      \n",
        "    # return tokens\n",
        "    sizes = []\n",
        "    x = tokens_list\n",
        "    while isinstance(x[0], list):\n",
        "        sizes.append(max(len(y) for y in x))\n",
        "        x = [z for y in x for z in y]\n",
        "    tokens = torch.LongTensor(batch_size, *sizes).fill_(0)\n",
        "    for i, s in enumerate(tokens_list):\n",
        "        tokens[i, :len(s)] = torch.LongTensor(s)\n",
        "    return tokens"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh930kSmAXFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loader:\n",
        "  \n",
        "  def __init__(self, file_path, batch_size):\n",
        "    self._file_path = file_path\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.data = self.load_file()\n",
        "    self.vocabs = self.get_vocabs(self.data)\n",
        "    self.data = self.preprocess(self.data, self.vocabs)\n",
        "    self.data = self.chunk_batches(self.data)\n",
        "\n",
        "  def load_file(self):\n",
        "    sents, sent = [], []\n",
        "    f = open(self._file_path)\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if len(line) == 0:\n",
        "        if len(sent) > 0:\n",
        "          sents.append(sent)\n",
        "          sent = []\n",
        "\n",
        "      else:\n",
        "        cols = line.split('\\t')\n",
        "        sent += [cols]\n",
        "    if len(sent) > 0:\n",
        "      sents.append(sent)\n",
        "\n",
        "    return sents\n",
        "\n",
        "  def get_vocabs(self, data):\n",
        "    words = [cols[FIELD_TO_IDX['word']] for sent in data for cols in sent]\n",
        "    postags = [cols[FIELD_TO_IDX['postag']] for sent in data for cols in sent] \n",
        "    deprels = [cols[FIELD_TO_IDX['deprel']] for sent in data for cols in sent] \n",
        "\n",
        "    words = self.build_vocab(words)\n",
        "    postags = self.build_vocab(postags)\n",
        "    deprels = self.build_vocab(deprels)\n",
        "\n",
        "    return {'words': words, 'postags': postags, 'deprels': deprels}\n",
        "\n",
        "  def build_vocab(self, cols):\n",
        "    w2i = {}\n",
        "    for w in cols:\n",
        "      if w not in w2i:\n",
        "        w2i[w] = len(w2i)\n",
        "    \n",
        "    return w2i\n",
        "\n",
        "  def preprocess(self, data, vocabs):\n",
        "    processed = []\n",
        "    for sent in data:\n",
        "      p_sent = [[vocabs['words'][w[FIELD_TO_IDX['word']]] for w in sent]]\n",
        "      p_sent += [[vocabs['postags'][w[FIELD_TO_IDX['postag']]] for w in sent]]\n",
        "      p_sent += [[int(w[FIELD_TO_IDX['head']]) for w in sent]]\n",
        "      p_sent += [[vocabs['deprels'][w[FIELD_TO_IDX['deprel']]] for w in sent]]\n",
        "      processed.append(p_sent)\n",
        "\n",
        "    return processed\n",
        "\n",
        "  def chunk_batches(self, data):\n",
        "    res, curr = [], []\n",
        "    currlen = 0\n",
        "    for sent in data:\n",
        "      if len(curr) >= self.batch_size:\n",
        "        res.append(curr)\n",
        "        curr = []\n",
        "        currlen = 0\n",
        "      \n",
        "      curr.append(sent)\n",
        "      currlen += len(sent[0])\n",
        "    if len(curr) >= self.batch_size:\n",
        "      res.append(curr)\n",
        "\n",
        "    return res\n",
        "    \n",
        "  def __getitem__(self, key):\n",
        "    batch = self.data[key]\n",
        "    batch = list(zip(*batch))\n",
        "    \n",
        "    # convert to tensors\n",
        "    words = batch[0]\n",
        "    words = get_long_tensor(words, len(words), vocabs=self.vocabs['words'])\n",
        "    words_mask = torch.eq(words, 0)\n",
        "\n",
        "    postags = batch[1]\n",
        "    postags = get_long_tensor(postags, len(postags), vocabs=self.vocabs['postags'])\n",
        "    \n",
        "    heads = batch[2]\n",
        "    heads = get_long_tensor(heads, len(heads))\n",
        "    \n",
        "    deprels = batch[3]\n",
        "    deprels = get_long_tensor(deprels, len(deprels), vocabs=self.vocabs['deprels'])\n",
        "    sentlens = [len(sent) for sent in batch[0]]\n",
        "\n",
        "    return words, words_mask, postags, heads, deprels, sentlens\n",
        "\n",
        "  def __iter__(self):\n",
        "    for i in range(len(self.data)):\n",
        "      yield self.__getitem__(i)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHafl7MzU6AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = Loader('test.txt', args['batch_size'])"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIkpLDVU-sX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "7778ced0-20c9-433d-f993-1b2a58809b37"
      },
      "source": [
        "data.vocabs"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'deprels': {'advmod': 2,\n",
              "  'appos': 6,\n",
              "  'nmod': 5,\n",
              "  'nsubj': 3,\n",
              "  'nummod': 0,\n",
              "  'obj': 1,\n",
              "  'punct': 7,\n",
              "  'root': 4},\n",
              " 'postags': {'ADV': 2, 'IN': 5, 'NN': 1, 'NUM': 0, 'PR': 3, 'SYM': 6, 'VB': 4},\n",
              " 'words': {'.': 14,\n",
              "  'Banyak': 0,\n",
              "  'Buku': 5,\n",
              "  'Kita': 8,\n",
              "  'baca': 4,\n",
              "  'berada': 10,\n",
              "  'buku': 1,\n",
              "  'dalam': 12,\n",
              "  'di': 11,\n",
              "  'dibaca': 7,\n",
              "  'ini': 6,\n",
              "  'ruangan': 13,\n",
              "  'saya': 3,\n",
              "  'sedang': 9,\n",
              "  'telah': 2}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWrYuzNo_D2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EES4WWjFwGO0",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ00FxbXAtSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepBiaffine(nn.Module):\n",
        "  def __init__(self, input1_size, input2_size, hidden_size, output_size, dropout=0.0):\n",
        "    super(DeepBiaffine, self).__init__()\n",
        "    # Simple MLP\n",
        "    self.W1 = nn.Linear(input1_size, hidden_size)\n",
        "    self.W2 = nn.Linear(input2_size, hidden_size)\n",
        "    self.relu = F.relu\n",
        "    \n",
        "    # Biaffine\n",
        "    self.biaff = nn.Bilinear(hidden_size + 1, hidden_size + 1, output_size)\n",
        "\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "    output1 = self.drop(self.relu(self.W1(input1)))\n",
        "    output2 = self.drop(self.relu(self.W2(input2)))\n",
        "    output1 = torch.cat([output1, output1.new_ones(*output1.size()[:-1], 1)], len(output1.size())-1)\n",
        "    output2 = torch.cat([output2, output2.new_ones(*output2.size()[:-1], 1)], len(output2.size())-1)\n",
        "    # print(output1.shape)\n",
        "    # output1 = output1.unsqueeze(len(output1.size())-1).expand(output1.size(0), output1.size(1), output1.size(1), output1.size(2))\n",
        "    # output2 = output2.unsqueeze(len(output2.size())-1).expand(output2.size(0), output2.size(1), output2.size(1), output2.size(2))\n",
        "\n",
        "    # print(output1.shape)\n",
        "\n",
        "    # return\n",
        "    return self.biaff(output1, output2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0doyE9pGmCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, args, vocab):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.args = args\n",
        "    self.vocab = vocab\n",
        "\n",
        "    # input layer\n",
        "    input_size = 0\n",
        "    self.word_emb = nn.Embedding(len(vocab['words']), self.args['word_emb_dim'])\n",
        "    input_size += self.args['word_emb_dim']\n",
        "    self.pos_emb = nn.Embedding(len(vocab['postags']), self.args['pos_emb_dim'])\n",
        "    input_size += self.args['pos_emb_dim']\n",
        "    self.dep_emb = nn.Embedding(len(vocab['deprels']), self.args['dep_emb_dim'])\n",
        "\n",
        "    # recurrent layer\n",
        "    self.GRU = nn.GRU(input_size, self.args['hidden_dim'], self.args['num_layers'],\\\n",
        "                            batch_first=True, dropout=self.args['dropout'], bidirectional=True)\n",
        "    self.GRU_hidden = nn.Parameter(torch.zeros(self.args['num_layers'] * 2, self.args['batch_size'], self.args['hidden_dim']))\n",
        "    \n",
        "    # classifier\n",
        "    self.unlabeled = DeepBiaffine(self.args['hidden_dim'] * 2, self.args['hidden_dim'] * 2, self.args['dba_hidden_dim'], 1, dropout=self.args['dropout'])\n",
        "    self.deprel = DeepBiaffine(self.args['hidden_dim'] * 2, self.args['hidden_dim'] * 2, self.args['dba_hidden_dim'], len(vocab['deprels']), dropout=self.args['dropout'])\n",
        "\n",
        "    # criterion\n",
        "    self.crit = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    \n",
        "    self.dropout = nn.Dropout(self.args['dropout'])\n",
        "  def forward(self, words, word_mask, postags, heads, deprels, sentlens):\n",
        "    #TODO: make forward func\n",
        "    inputs = []\n",
        "\n",
        "    # pack embedded inputs\n",
        "    embedded_word = self.word_emb(words)\n",
        "    inputs += [pack_padded_sequence(embedded_word, sentlens, batch_first=True)]\n",
        "\n",
        "    embedded_pos = self.pos_emb(postags)\n",
        "    inputs += [pack_padded_sequence(embedded_pos, sentlens, batch_first=True)]\n",
        "\n",
        "    # rnn inputs\n",
        "    rnn_inputs = torch.cat([x.data for x in inputs], 1)\n",
        "    rnn_inputs = self.dropout(rnn_inputs)\n",
        "    rnn_inputs = PackedSequence(rnn_inputs, inputs[0].batch_sizes)\n",
        "\n",
        "    rnn_outputs, _ = self.GRU(rnn_inputs, self.GRU_hidden)\n",
        "    rnn_outputs, _ = pad_packed_sequence(rnn_outputs, batch_first=True)\n",
        "    # print(\"rnn \", rnn_outputs.shape)\n",
        "    unlabeled_inputs = rnn_outputs.unsqueeze(len(rnn_outputs.size())-1).expand(rnn_outputs.size(0), rnn_outputs.size(1), rnn_outputs.size(1), rnn_outputs.size(2))\n",
        "    # print(\"rnn1 \", rnn_outputs.shape)\n",
        "\n",
        "    #TODO: classify outputs, calc loss\n",
        "    unlabeled_scores = self.unlabeled(self.dropout(unlabeled_inputs), self.dropout(unlabeled_inputs))\n",
        "    deprel_scores = self.deprel(self.dropout(rnn_outputs), self.dropout(rnn_outputs))\n",
        "    \n",
        "    # unlabeled_scores = unlabeled_scores[:, 1:, :]\n",
        "    unlabeled_scores = unlabeled_scores.contiguous().view(-1, unlabeled_scores.size(2))\n",
        "    deprel_scores = deprel_scores.contiguous().view(-1, len(self.vocab['deprels']))\n",
        "\n",
        "    heads -= 1\n",
        "    preds = []\n",
        "    if self.training:\n",
        "      loss = self.crit(unlabeled_scores, heads.view(-1))\n",
        "      loss += self.crit(deprel_scores, deprels.view(-1))\n",
        "      \n",
        "      # calculate accuracy\n",
        "      unlabeled_preds = F.log_softmax(unlabeled_scores, 1)\n",
        "      unlabeled_preds = unlabeled_preds.argmax(dim=1)\n",
        "      ul_corr = (unlabeled_preds == heads.view(-1))\n",
        "      acc1 = ul_corr.sum().float() / float( heads.view(-1).size(0) )\n",
        "\n",
        "      deprel_preds = F.log_softmax(deprel_scores, 1)\n",
        "      deprel_preds = deprel_scores.argmax(dim=1)\n",
        "      dep_corr = (deprel_preds == deprels.view(-1))\n",
        "      acc2 = dep_corr.sum().float() / float( deprels.view(-1).size(0) )\n",
        "\n",
        "      acc = (acc1 + acc2)/2\n",
        "    else:\n",
        "      loss = self.crit(unlabeled_scores, heads.view(-1))\n",
        "      loss += self.crit(deprel_scores, deprels.view(-1))\n",
        "\n",
        "      # calculate accuracy\n",
        "      unlabeled_preds = F.log_softmax(unlabeled_scores, 1)\n",
        "      unlabeled_preds = unlabeled_preds.argmax(dim=1)\n",
        "      ul_corr = (unlabeled_preds == heads.view(-1))\n",
        "      acc1 = ul_corr.sum().float() / float( heads.view(-1).size(0) )\n",
        "\n",
        "      deprel_preds = F.log_softmax(deprel_scores, 1)\n",
        "      deprel_preds = deprel_scores.argmax(dim=1)\n",
        "      dep_corr = (deprel_preds == deprels.view(-1))\n",
        "      acc2 = dep_corr.sum().float() / float( deprels.view(-1).size(0) )\n",
        "\n",
        "      acc = (acc1 + acc2)/2\n",
        "\n",
        "      # predictions\n",
        "      unlabeled_preds = F.log_softmax(unlabeled_scores, 1)\n",
        "      unlabeled_preds = unlabeled_preds.argmax(dim=1)\n",
        "      unlabeled_preds += 1\n",
        "\n",
        "      deprel_preds = F.log_softmax(deprel_scores, 1)\n",
        "      deprel_preds = deprel_scores.argmax(dim=1)\n",
        "\n",
        "      preds.append(unlabeled_preds.detach().cpu().numpy())\n",
        "      preds.append(deprel_preds.detach().cpu().numpy())\n",
        "    return loss, acc, preds"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzZt0XO12kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(args, data.vocabs)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1TrIexi0Dg7",
        "colab_type": "text"
      },
      "source": [
        "# Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVntPapSyU9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def train_model(args, model, train_data, eval_data, num_epochs):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=args['learning_rate'])\n",
        "  print('start training...')\n",
        "  start_time = time.time()\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    # train\n",
        "    train_loss = train_acc = 0\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_data):\n",
        "      model.zero_grad()\n",
        "\n",
        "      words, word_mask, postags, heads, deprels, sentlens = batch\n",
        "      loss, acc, _ = model(words, word_mask, postags, heads, deprels, sentlens)\n",
        "      train_loss += loss.data.cpu().numpy()\n",
        "      train_acc += acc.data.cpu().numpy()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
        "      optimizer.step()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    train_loss /= len(train_data)\n",
        "    train_acc /= len(train_data)\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "    eval_loss = eval_acc = 0\n",
        "    for i, batch in enumerate(eval_data):\n",
        "      model.zero_grad()\n",
        "\n",
        "      words, word_mask, postags, heads, deprels, sentlens = batch\n",
        "      loss, acc, _ = model(words, word_mask, postags, heads, deprels, sentlens)\n",
        "      eval_loss += loss.data.cpu().numpy()\n",
        "      eval_acc += acc.data.cpu().numpy()\n",
        "\n",
        "    eval_loss /= len(eval_data)\n",
        "    eval_acc /= len(eval_data)\n",
        "\n",
        "    log = '|  {}/{} epoch  |  train_loss:{:.5f} | train_acc:{:2.2f} |  eval_loss:{:.5f} | eval_acc:{:2.2f} | time: {:.2f}  |'.format(\n",
        "        epoch, num_epochs, train_loss, train_acc * 100, eval_loss, eval_acc * 100, elapsed_time\n",
        "    )\n",
        "\n",
        "    print(log)\n",
        "  return"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEVCYDrAKPqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0ad59ec4-14e7-424b-ce44-e29af4c84729"
      },
      "source": [
        "train_model(args, model, data, data, 3)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training...\n",
            "|  0/3 epoch  |  train_loss:3.66341 | train_acc:23.93 |  eval_loss:3.66762 | eval_acc:30.60 | time: 0.46  |\n",
            "|  1/3 epoch  |  train_loss:3.64233 | train_acc:27.86 |  eval_loss:3.65619 | eval_acc:33.93 | time: 0.45  |\n",
            "|  2/3 epoch  |  train_loss:3.67362 | train_acc:18.21 |  eval_loss:3.64528 | eval_acc:33.93 | time: 0.46  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pSP5y3X6oyt",
        "colab_type": "text"
      },
      "source": [
        "# Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp_6517C6mP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sentence):\n",
        "  #TODO: sentence func\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdtQxJxK601C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}